{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 Optimisation modèle : Réclamation aux Etats Unis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import de librairies et lecture du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairie pour la manipulation des dataframes\n",
    "import pandas as pd\n",
    "# librairie pour la manipulation des tableaux (array)\n",
    "import numpy as np\n",
    "\n",
    "# libraires (os & glob) pour la manipulation de l'os (systeme d'exploitatin). Elle permet de se balader dans l'ordinateur\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# gc est le garbage collector (pr gérer la mémoire vive de son pc), c'est très important quand on multiplie le nombre de table\n",
    "import gc\n",
    "\n",
    "# sklearn : scikit-learn est la librairie utilisé par la majorité des utilisateurs pour toutes les fonctionnalités de base de datascience.\n",
    "#elle contient beaucoup de fonctionnalité très pratique\n",
    "# preprocessing : La normalisation ou la standardisation\n",
    "from sklearn import preprocessing\n",
    "#Metrics permet de programmer automatiquement toutes les métriques usuelles\n",
    "from sklearn import metrics\n",
    "\n",
    "# train_test_split => Fonctionnalité très utile pour créer ses datasets de test et de train\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# xgboost => librairie optimisé utilisant le model de descente du gradient optimisé: http://xgboost.readthedocs.io/en/latest/model.html \n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraire de NLP classic (spacy est plus à la mode aujourd'hui)\n",
    "import nltk\n",
    "#utile pour \"tokenizer\" les zones de texte : rappel tokenizer('Bonjour, comment allez vous?') = [Bonjour, comment, allez, vous]\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "#utile pour le pos_tagging ou lemmatizer (corpus définit les dictionnaires de vocabulaire créer par les auteurs de nltk)\n",
    "from nltk.corpus import wordnet as wn\n",
    "# import le dictionnaire de mot usuel qui n'ont pas vocation à pouvoir permettre d'apporter de l'information pour la prise de décision\n",
    "from nltk.corpus import stopwords\n",
    "# lemmatizer(remove_stop_word(nous avons beaucoup de travaux à finir) = beaucoup travail finir\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#création d'une instance de tokenization\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# création d'une instance de lemmatization\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation d'une variable de contexte pour le folder\n",
    "s_working_directory = '/Users/moi/Documents/CoursDauphine/module3/projects/finance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# téléchargement de tous les mots packages de base pour nltk\n",
    "# nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run du fichier toolkit : \n",
    "%run \"/Users/moi/Documents/CoursDauphine/module3/projects/finance/TP3 FI/toolkit.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture de la table des variables créer pendant le TP2 et réapplication de la fonction de la lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complaint = pd.read_csv(s_working_directory + '/us-consumer-finance-complaint-database/TP_2_var_consumer_complaints.csv', sep=';', decimal='.',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>date_received</th>\n",
       "      <th>product</th>\n",
       "      <th>sub_product</th>\n",
       "      <th>issue</th>\n",
       "      <th>sub_issue</th>\n",
       "      <th>consumer_complaint_narrative</th>\n",
       "      <th>company_public_response</th>\n",
       "      <th>company</th>\n",
       "      <th>state</th>\n",
       "      <th>...</th>\n",
       "      <th>var_6_nb_reclamation_state_sub_issue</th>\n",
       "      <th>var_7_nb_reclamation_state_sub_product</th>\n",
       "      <th>mean|var_1_bis_taille_reclamation_sub_product</th>\n",
       "      <th>std|var_1_bis_taille_reclamation_sub_product</th>\n",
       "      <th>var_8_distrib_normal_var_1_x_sub_produit</th>\n",
       "      <th>mean|var_1_bis_taille_reclamation_sub_issue</th>\n",
       "      <th>std|var_1_bis_taille_reclamation_sub_issue</th>\n",
       "      <th>var_9_distrib_normal_var_1_x_sub_issue</th>\n",
       "      <th>var_10_nb_XXXX</th>\n",
       "      <th>var_11_nb_$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>190126</td>\n",
       "      <td>2015-03-19</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Other (i.e. phone, health club, etc.)</td>\n",
       "      <td>Cont'd attempts collect debt not owed</td>\n",
       "      <td>Debt was paid</td>\n",
       "      <td>XXXX has claimed I owe them {$27.00} for XXXX ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Diversified Consultants, Inc.</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>105.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>60.811156</td>\n",
       "      <td>44.655486</td>\n",
       "      <td>-0.300200</td>\n",
       "      <td>59.765306</td>\n",
       "      <td>41.898063</td>\n",
       "      <td>-0.307476</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>196219</td>\n",
       "      <td>2015-03-26</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Other (i.e. phone, health club, etc.)</td>\n",
       "      <td>Cont'd attempts collect debt not owed</td>\n",
       "      <td>Debt was paid</td>\n",
       "      <td>I have informed credit reporting agencies abou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hunter Warfield, Inc.</td>\n",
       "      <td>HI</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>60.811156</td>\n",
       "      <td>44.655486</td>\n",
       "      <td>0.394004</td>\n",
       "      <td>59.765306</td>\n",
       "      <td>41.898063</td>\n",
       "      <td>0.432415</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>197286</td>\n",
       "      <td>2015-03-26</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Other (i.e. phone, health club, etc.)</td>\n",
       "      <td>Cont'd attempts collect debt not owed</td>\n",
       "      <td>Debt was paid</td>\n",
       "      <td>I resided at XXXX from XXXX ( see attached ), ...</td>\n",
       "      <td>Company believes complaint is the result of an...</td>\n",
       "      <td>First Advantage LNS Inc.</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>200.0</td>\n",
       "      <td>592.0</td>\n",
       "      <td>60.811156</td>\n",
       "      <td>44.655486</td>\n",
       "      <td>-0.210625</td>\n",
       "      <td>59.765306</td>\n",
       "      <td>41.898063</td>\n",
       "      <td>-0.212006</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>197411</td>\n",
       "      <td>2015-03-20</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Other (i.e. phone, health club, etc.)</td>\n",
       "      <td>Cont'd attempts collect debt not owed</td>\n",
       "      <td>Debt was paid</td>\n",
       "      <td>XXXX apartments is trying to collect an amount...</td>\n",
       "      <td>Company believes it acted appropriately as aut...</td>\n",
       "      <td>I.Q. Data International, Inc.</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>300.0</td>\n",
       "      <td>681.0</td>\n",
       "      <td>60.811156</td>\n",
       "      <td>44.655486</td>\n",
       "      <td>0.304429</td>\n",
       "      <td>59.765306</td>\n",
       "      <td>41.898063</td>\n",
       "      <td>0.336945</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>199217</td>\n",
       "      <td>2015-03-27</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Other (i.e. phone, health club, etc.)</td>\n",
       "      <td>Cont'd attempts collect debt not owed</td>\n",
       "      <td>Debt was paid</td>\n",
       "      <td>XXXX has sent a previous account into collecti...</td>\n",
       "      <td>Company believes it acted appropriately as aut...</td>\n",
       "      <td>Diversified Consultants, Inc.</td>\n",
       "      <td>AL</td>\n",
       "      <td>...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>60.811156</td>\n",
       "      <td>44.655486</td>\n",
       "      <td>-0.423365</td>\n",
       "      <td>59.765306</td>\n",
       "      <td>41.898063</td>\n",
       "      <td>-0.438747</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index date_received          product  \\\n",
       "0  190126    2015-03-19  Debt collection   \n",
       "1  196219    2015-03-26  Debt collection   \n",
       "2  197286    2015-03-26  Debt collection   \n",
       "3  197411    2015-03-20  Debt collection   \n",
       "4  199217    2015-03-27  Debt collection   \n",
       "\n",
       "                             sub_product  \\\n",
       "0  Other (i.e. phone, health club, etc.)   \n",
       "1  Other (i.e. phone, health club, etc.)   \n",
       "2  Other (i.e. phone, health club, etc.)   \n",
       "3  Other (i.e. phone, health club, etc.)   \n",
       "4  Other (i.e. phone, health club, etc.)   \n",
       "\n",
       "                                   issue      sub_issue  \\\n",
       "0  Cont'd attempts collect debt not owed  Debt was paid   \n",
       "1  Cont'd attempts collect debt not owed  Debt was paid   \n",
       "2  Cont'd attempts collect debt not owed  Debt was paid   \n",
       "3  Cont'd attempts collect debt not owed  Debt was paid   \n",
       "4  Cont'd attempts collect debt not owed  Debt was paid   \n",
       "\n",
       "                        consumer_complaint_narrative  \\\n",
       "0  XXXX has claimed I owe them {$27.00} for XXXX ...   \n",
       "1  I have informed credit reporting agencies abou...   \n",
       "2  I resided at XXXX from XXXX ( see attached ), ...   \n",
       "3  XXXX apartments is trying to collect an amount...   \n",
       "4  XXXX has sent a previous account into collecti...   \n",
       "\n",
       "                             company_public_response  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  Company believes complaint is the result of an...   \n",
       "3  Company believes it acted appropriately as aut...   \n",
       "4  Company believes it acted appropriately as aut...   \n",
       "\n",
       "                         company state     ...       \\\n",
       "0  Diversified Consultants, Inc.    NY     ...        \n",
       "1          Hunter Warfield, Inc.    HI     ...        \n",
       "2       First Advantage LNS Inc.    TX     ...        \n",
       "3  I.Q. Data International, Inc.    CA     ...        \n",
       "4  Diversified Consultants, Inc.    AL     ...        \n",
       "\n",
       "  var_6_nb_reclamation_state_sub_issue var_7_nb_reclamation_state_sub_product  \\\n",
       "0                                105.0                                  237.0   \n",
       "1                                  3.0                                   12.0   \n",
       "2                                200.0                                  592.0   \n",
       "3                                300.0                                  681.0   \n",
       "4                                 28.0                                   66.0   \n",
       "\n",
       "  mean|var_1_bis_taille_reclamation_sub_product  \\\n",
       "0                                     60.811156   \n",
       "1                                     60.811156   \n",
       "2                                     60.811156   \n",
       "3                                     60.811156   \n",
       "4                                     60.811156   \n",
       "\n",
       "  std|var_1_bis_taille_reclamation_sub_product  \\\n",
       "0                                    44.655486   \n",
       "1                                    44.655486   \n",
       "2                                    44.655486   \n",
       "3                                    44.655486   \n",
       "4                                    44.655486   \n",
       "\n",
       "  var_8_distrib_normal_var_1_x_sub_produit  \\\n",
       "0                                -0.300200   \n",
       "1                                 0.394004   \n",
       "2                                -0.210625   \n",
       "3                                 0.304429   \n",
       "4                                -0.423365   \n",
       "\n",
       "  mean|var_1_bis_taille_reclamation_sub_issue  \\\n",
       "0                                   59.765306   \n",
       "1                                   59.765306   \n",
       "2                                   59.765306   \n",
       "3                                   59.765306   \n",
       "4                                   59.765306   \n",
       "\n",
       "  std|var_1_bis_taille_reclamation_sub_issue  \\\n",
       "0                                  41.898063   \n",
       "1                                  41.898063   \n",
       "2                                  41.898063   \n",
       "3                                  41.898063   \n",
       "4                                  41.898063   \n",
       "\n",
       "  var_9_distrib_normal_var_1_x_sub_issue  var_10_nb_XXXX  var_11_nb_$  \n",
       "0                              -0.307476               2            2  \n",
       "1                               0.432415               9            2  \n",
       "2                              -0.212006               7            2  \n",
       "3                               0.336945               5            2  \n",
       "4                              -0.438747               1            2  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complaint.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "drop() got an unexpected keyword argument 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d501e7bf74d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_complaint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ins_consumer_complaint_narrative_stop_words_lemmatized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# application du mask pour filter puis reinitialisation de l'index (voir dans le fichier .py)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf_complaint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reinitialize_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_complaint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/CoursDauphine/module3/projects/finance/TP3 FI/toolkit.py\u001b[0m in \u001b[0;36m_reinitialize_df\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     11\u001b[0m \tréinitialisation de la table (retour de l'index de 0 à len(df) - 1\n\u001b[1;32m     12\u001b[0m \t'''\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_preprocessing_nlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: drop() got an unexpected keyword argument 'columns'"
     ]
    }
   ],
   "source": [
    "# replace _ par espace (contenu dans 89 lignes)\n",
    "df_complaint['ins_consumer_complaint_narrative_stop_words_lemmatized'] = df_complaint['ins_consumer_complaint_narrative_stop_words_lemmatized'].str.replace('_', ' ')\n",
    "# mask excluant les textes vide après exclusion des stops words\n",
    "mask = df_complaint['ins_consumer_complaint_narrative_stop_words_lemmatized'].notnull()\n",
    "# application du mask pour filter puis reinitialisation de l'index (voir dans le fichier .py)\n",
    "df_complaint = _reinitialize_df(df_complaint[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_complaint['ins_consumer_complaint_narrative_stop_words_lemmatized'] = df_complaint['ins_consumer_complaint_narrative_stop_words_lemmatized'].apply(_lemmatize)\n",
    "df_complaint = _reinitialize_df(df_complaint[~df_complaint['ins_consumer_complaint_narrative_stop_words_lemmatized'].str.contains('_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ins_Word</th>\n",
       "      <th>Ins_Nb_Appearance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>credit</td>\n",
       "      <td>89200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>account</td>\n",
       "      <td>84975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>call</td>\n",
       "      <td>72009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>payment</td>\n",
       "      <td>69291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pay</td>\n",
       "      <td>57704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>loan</td>\n",
       "      <td>55537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>report</td>\n",
       "      <td>53244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bank</td>\n",
       "      <td>45944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tell</td>\n",
       "      <td>45268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>receive</td>\n",
       "      <td>43279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>time</td>\n",
       "      <td>42277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>make</td>\n",
       "      <td>40307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>get</td>\n",
       "      <td>39712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>debt</td>\n",
       "      <td>37397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>say</td>\n",
       "      <td>35087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>card</td>\n",
       "      <td>32952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mortgage</td>\n",
       "      <td>32430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>company</td>\n",
       "      <td>32369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>information</td>\n",
       "      <td>31435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>send</td>\n",
       "      <td>31206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>day</td>\n",
       "      <td>28354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>month</td>\n",
       "      <td>28102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>state</td>\n",
       "      <td>27911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>go</td>\n",
       "      <td>27499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>letter</td>\n",
       "      <td>27378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>charge</td>\n",
       "      <td>27305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ask</td>\n",
       "      <td>26973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>year</td>\n",
       "      <td>26337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>never</td>\n",
       "      <td>26121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>contact</td>\n",
       "      <td>25784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40389</th>\n",
       "      <td>fifferent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40390</th>\n",
       "      <td>deposi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40391</th>\n",
       "      <td>masterd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40392</th>\n",
       "      <td>abill</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40393</th>\n",
       "      <td>greendot</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40394</th>\n",
       "      <td>direcr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40395</th>\n",
       "      <td>reflecta</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40396</th>\n",
       "      <td>edible</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40397</th>\n",
       "      <td>cracker</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40398</th>\n",
       "      <td>occording</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40399</th>\n",
       "      <td>referal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40400</th>\n",
       "      <td>foretell</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40401</th>\n",
       "      <td>unneed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40402</th>\n",
       "      <td>hunged</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40403</th>\n",
       "      <td>reportage</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40404</th>\n",
       "      <td>communitcating</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40405</th>\n",
       "      <td>vehicel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40406</th>\n",
       "      <td>resdience</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40407</th>\n",
       "      <td>commuincation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40408</th>\n",
       "      <td>genreal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40409</th>\n",
       "      <td>fargothe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40410</th>\n",
       "      <td>devited</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40411</th>\n",
       "      <td>electroically</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40412</th>\n",
       "      <td>requried</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40413</th>\n",
       "      <td>attemoting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40414</th>\n",
       "      <td>recorse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40415</th>\n",
       "      <td>responsibity</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40416</th>\n",
       "      <td>resisued</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40417</th>\n",
       "      <td>travelex</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40418</th>\n",
       "      <td>montero</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40419 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Ins_Word  Ins_Nb_Appearance\n",
       "0              credit              89200\n",
       "1             account              84975\n",
       "2                call              72009\n",
       "3             payment              69291\n",
       "4                 pay              57704\n",
       "5                loan              55537\n",
       "6              report              53244\n",
       "7                bank              45944\n",
       "8                tell              45268\n",
       "9             receive              43279\n",
       "10               time              42277\n",
       "11               make              40307\n",
       "12                get              39712\n",
       "13               debt              37397\n",
       "14                say              35087\n",
       "15               card              32952\n",
       "16           mortgage              32430\n",
       "17            company              32369\n",
       "18        information              31435\n",
       "19               send              31206\n",
       "20                day              28354\n",
       "21              month              28102\n",
       "22              state              27911\n",
       "23                 go              27499\n",
       "24             letter              27378\n",
       "25             charge              27305\n",
       "26                ask              26973\n",
       "27               year              26337\n",
       "28              never              26121\n",
       "29            contact              25784\n",
       "...               ...                ...\n",
       "40389       fifferent                  1\n",
       "40390          deposi                  1\n",
       "40391         masterd                  1\n",
       "40392           abill                  1\n",
       "40393        greendot                  1\n",
       "40394          direcr                  1\n",
       "40395        reflecta                  1\n",
       "40396          edible                  1\n",
       "40397         cracker                  1\n",
       "40398       occording                  1\n",
       "40399         referal                  1\n",
       "40400        foretell                  1\n",
       "40401          unneed                  1\n",
       "40402          hunged                  1\n",
       "40403       reportage                  1\n",
       "40404  communitcating                  1\n",
       "40405         vehicel                  1\n",
       "40406       resdience                  1\n",
       "40407   commuincation                  1\n",
       "40408         genreal                  1\n",
       "40409        fargothe                  1\n",
       "40410         devited                  1\n",
       "40411   electroically                  1\n",
       "40412        requried                  1\n",
       "40413      attemoting                  1\n",
       "40414         recorse                  1\n",
       "40415    responsibity                  1\n",
       "40416        resisued                  1\n",
       "40417        travelex                  1\n",
       "40418         montero                  1\n",
       "\n",
       "[40419 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Création d'une liste ou chaque élement est le commentaire d'une réclamation\n",
    "list_words_content = df_complaint.ins_consumer_complaint_narrative_stop_words_lemmatized.tolist()\n",
    "# Création d'une liste de tous les mots des commentaires juxtaposé les un avec les autres \n",
    "list_words_all_content = ' '.join(tokenizer.tokenize(' '.join(list_words_content).lower().replace(',',' ').replace('-',' ')))\n",
    "# Création du contenu tokenizer de la liste\n",
    "all_content_tokenized = tokenizer.tokenize(list_words_all_content)\n",
    "# Création de la table contenant tous les mots avec leur fréquence\n",
    "df_sorted_content = _vocabulary_frequence(all_content_tokenized)\n",
    "# apparition de la table\n",
    "df_sorted_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40419\n",
      "5273387\n"
     ]
    }
   ],
   "source": [
    "set_words = set(all_content_tokenized)\n",
    "print(len(set_words))\n",
    "print(len(all_content_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de la table intermédiaire pour l'analyse des mots d'importance pour la prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création du y_output avec 1 si Oui 0 sinon\n",
    "df_complaint['y_output'] = np.where(df_complaint['consumer_disputed?'] == 'Yes', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Petite astuce pour créer les colonnes automatiquement issue d'un split entre les mots\n",
    "l_col_name = ['word_' + str(value) for value in pd.DataFrame(df_complaint['ins_consumer_complaint_narrative_stop_words_lemmatized'].str.split(' ').tolist()).columns]\n",
    "# creation d'une table temporaire qui va contenir une colonne par mots de la réclamation et le y_output ainsi que le y output\n",
    "df_tmp = df_complaint[['y_output', 'complaint_id']].merge(pd.DataFrame(df_complaint['ins_consumer_complaint_narrative_stop_words_lemmatized'].str.split(' ').tolist(), columns=l_col_name), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melt = pd.melt(df_tmp, id_vars=['complaint_id', 'y_output'], var_name='variable_name', value_vars=l_col_name, value_name='word')\n",
    "df_melt = df_melt[df_melt.word.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "436"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum|y_output</th>\n",
       "      <th>len|y_output</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aa</th>\n",
       "      <td>19</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaa</th>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaked</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaarm</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aac</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aacceptance</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaccount</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaccounts</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aacount</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaddress</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aadhar</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aadvantage</th>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aafes</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aafraid</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aag</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aagain</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aagon</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aai</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaliant</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aallc</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aallcdelete</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aamerican</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aand</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aapproved</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aarea</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aargon</th>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaround</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aarp</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aasigned</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zager</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zakheim</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zale</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zales</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zap</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zealous</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zed</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zenco</th>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zenoco</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>24</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zerobalance</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zerord</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zest</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zfn</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zgoggan</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zi</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zia</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zilch</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zinco</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zion</th>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zip</th>\n",
       "      <td>38</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipcode</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zipper</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zombie</th>\n",
       "      <td>14</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>25</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoo</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoom</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ztuff</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zwicker</th>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>â</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40419 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sum|y_output  len|y_output\n",
       "word                                   \n",
       "a                       3             9\n",
       "aa                     19            40\n",
       "aaa                     4            21\n",
       "aaaked                  0             1\n",
       "aaarm                   0             2\n",
       "aac                     0             1\n",
       "aacceptance             0             1\n",
       "aaccount                0             2\n",
       "aaccounts               0             1\n",
       "aacount                 1             2\n",
       "aaddress                4             6\n",
       "aadhar                  0             1\n",
       "aadvantage              6            34\n",
       "aafes                   3            16\n",
       "aafraid                 0             1\n",
       "aag                     1             5\n",
       "aagain                  0             1\n",
       "aagon                   0             1\n",
       "aai                     0             2\n",
       "aaliant                 1             1\n",
       "aallc                   5             5\n",
       "aallcdelete             1             1\n",
       "aamerican               1             1\n",
       "aand                    2             8\n",
       "aapproved               1             1\n",
       "aarea                   0             1\n",
       "aargon                 15            45\n",
       "aaround                 1             1\n",
       "aarp                    3             9\n",
       "aasigned                0             1\n",
       "...                   ...           ...\n",
       "zager                   1             1\n",
       "zakheim                 0             8\n",
       "zale                    0             1\n",
       "zales                   0            21\n",
       "zap                     0             4\n",
       "zealous                 0             1\n",
       "zed                     0             8\n",
       "zenco                   5            14\n",
       "zenoco                  0             1\n",
       "zero                   24           122\n",
       "zerobalance             1             1\n",
       "zerord                  1             1\n",
       "zest                    1             3\n",
       "zfn                     2             7\n",
       "zgoggan                 0             2\n",
       "zi                      0             1\n",
       "zia                     0             2\n",
       "zilch                   0             1\n",
       "zinco                   0             1\n",
       "zion                   15            32\n",
       "zip                    38           107\n",
       "zipcode                 1             4\n",
       "zipper                  0             2\n",
       "zombie                 14            51\n",
       "zone                   25            92\n",
       "zoo                     0             1\n",
       "zoom                    0             2\n",
       "ztuff                   0             1\n",
       "zwicker                 6            14\n",
       "â                       0             3\n",
       "\n",
       "[40419 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# création d'un pivot table avec deux fonctions d'aggrégation : somme et nombre de valeurs\n",
    "df_pivot = pd.pivot_table(df_melt, values='y_output', index='word', aggfunc=[np.sum, len])\n",
    "# peite astuce pour combien le nom des colonnes afin de ne pas avoir de problème lors du reset_index\n",
    "df_pivot.columns = ['%s%s' % (a, '|%s' % b if b else '') for a, b in df_pivot.columns]\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_index afin d'éviter les complexités de manipulation de niveau des axes (pour les colonnes) et de l'index ligne qui sera le mot\n",
    "df_pivot = df_pivot.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour renommer les colonnes\n",
    "df_pivot = df_pivot.rename(columns={'sum|y_output' : 'nb_yes', \n",
    "                                    'len|y_output' : 'nb_occurence'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot['ins_ratio_yes'] = df_pivot['nb_yes'] / df_pivot['nb_occurence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée de construire ce lexique est de pouvoir compter le nombre de mots de ce lexique (sous forme de combinaison) qui sera présent dans la réclamation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Définition du lexique de mots qui vont être considéré comme n'entrainant pas de réclamations\n",
    "l_no_dispute = df_pivot[(df_pivot['ins_ratio_yes'] < 0.05)]['word'].tolist()\n",
    "set_no_dispute = set(l_no_dispute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée de construire ce lexique est de pouvoir compter le nombre de mots de ce lexique (sous forme de combinaison) qui sera présent dans la réclamation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l_disputed = df_pivot[(df_pivot['ins_ratio_yes'] > 0.35)]['word'].tolist() \n",
    "set_disputed = set(l_disputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pas utilisé au final car n'a pas apporté de valeurs\n",
    "l_all_word_50 = df_pivot[(df_pivot['nb_occurence'] > 50)]['word'].tolist()\n",
    "set_all_word_50 = set(l_all_word_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des nouvelles variables pour l'utilisation du modèle: \n",
    "df_complaint['var12_nb_word_dispute'] = df_complaint['ins_consumer_complaint_narrative_stop_words_lemmatized'].apply(_nb_word_dispute)\n",
    "df_complaint['var13_nb_word_different'] = df_complaint['ins_consumer_complaint_narrative_stop_words_lemmatized'].apply(lambda x : len(set(tokenizer.tokenize(x))))\n",
    "df_complaint['va14_ratio_word_dispute_vs_all'] =  df_complaint['var12_nb_word_dispute'] / df_complaint['var13_nb_word_different'] \n",
    "df_complaint['var15_nb_word_positif'] = df_complaint['ins_consumer_complaint_narrative_stop_words_lemmatized'].apply(lambda x : len(set(tokenizer.tokenize(x)).intersection(set_no_dispute)))\n",
    "df_complaint['va16_ratio_word_positif_vs_all'] =  df_complaint['var15_nb_word_positif'] / df_complaint['var13_nb_word_different']\n",
    "df_complaint['va17_ratio_word_negatif_vs_word_positif'] = df_complaint['var12_nb_word_dispute'] / np.where(df_complaint['var15_nb_word_positif'] == 0, 1, df_complaint['var15_nb_word_positif'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complaint[['var12_nb_word_dispute', 'var13_nb_word_different', 'va14_ratio_word_dispute_vs_all', 'var15_nb_word_positif', 'va16_ratio_word_positif_vs_all', 'va17_ratio_word_negatif_vs_word_positif', 'y_output']].to_csv(s_working_directory+ '/data.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèlisation :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### création des deux datasets (un avec les nouvelles variables et un avec ceux de base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date_received', 'product', 'sub_product', 'issue', 'sub_issue',\n",
       "       'consumer_complaint_narrative', 'company_public_response', 'company',\n",
       "       'state', 'zipcode', 'tags', 'consumer_consent_provided',\n",
       "       'submitted_via', 'date_sent_to_company', 'company_response_to_consumer',\n",
       "       'timely_response', 'consumer_disputed?', 'complaint_id', 'tec_count',\n",
       "       'ins_consumer_complaint_narrative_stop_words',\n",
       "       'ins_consumer_complaint_narrative_stop_words_lemmatized',\n",
       "       'var_1_taille_reclamation', 'var_1_bis_taille_reclamation',\n",
       "       'var_2_nb_reclamation_company',\n",
       "       'var_3_nb_reclamation_company_sub_product',\n",
       "       'var_4_nb_reclamation_company_sub_issue',\n",
       "       'var_5_nb_reclamation_company_state',\n",
       "       'var_6_nb_reclamation_state_sub_issue',\n",
       "       'var_7_nb_reclamation_state_sub_product',\n",
       "       'mean|var_1_bis_taille_reclamation_sub_product',\n",
       "       'std|var_1_bis_taille_reclamation_sub_product',\n",
       "       'var_8_distrib_normal_var_1_x_sub_produit',\n",
       "       'mean|var_1_bis_taille_reclamation_sub_issue',\n",
       "       'std|var_1_bis_taille_reclamation_sub_issue',\n",
       "       'var_9_distrib_normal_var_1_x_sub_issue', 'var_10_nb_XXXX',\n",
       "       'var_11_nb_$', 'y_output', 'var12_nb_word_dispute',\n",
       "       'var13_nb_word_different', 'va14_ratio_word_dispute_vs_all',\n",
       "       'var15_nb_word_positif', 'va16_ratio_word_positif_vs_all',\n",
       "       'va17_ratio_word_negatif_vs_word_positif'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complaint.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_complaint.to_excel(s_working_directory + '_data/analyse_complaints/TP_2_var_consumer_complaints.xlsx',encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une instance de standardisation : \n",
    "std_scaler = preprocessing.StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[[]] permet de faire une sous sélection de variable pour une nouvelle table\n",
    "df_complaint_dataset_1 = df_complaint[['var_1_bis_taille_reclamation', \n",
    "                                       'var_8_distrib_normal_var_1_x_sub_produit', \n",
    "                                       'var_9_distrib_normal_var_1_x_sub_issue', \n",
    "                                       'var_10_nb_XXXX', \n",
    "                                       'var_11_nb_$', \n",
    "                                       'var12_nb_word_dispute', \n",
    "                                       'var13_nb_word_different', \n",
    "                                       'va14_ratio_word_dispute_vs_all', \n",
    "                                       'var15_nb_word_positif', \n",
    "                                       'va16_ratio_word_positif_vs_all', \n",
    "                                       'va17_ratio_word_negatif_vs_word_positif']]\n",
    "#permet de remplacer les valeurs vides par 0\n",
    "df_complaint_dataset_1 = df_complaint_dataset_1.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_1_bis_taille_reclamation</th>\n",
       "      <th>var_8_distrib_normal_var_1_x_sub_produit</th>\n",
       "      <th>var_9_distrib_normal_var_1_x_sub_issue</th>\n",
       "      <th>var_10_nb_XXXX</th>\n",
       "      <th>var_11_nb_$</th>\n",
       "      <th>var12_nb_word_dispute</th>\n",
       "      <th>var13_nb_word_different</th>\n",
       "      <th>va14_ratio_word_dispute_vs_all</th>\n",
       "      <th>var15_nb_word_positif</th>\n",
       "      <th>va16_ratio_word_positif_vs_all</th>\n",
       "      <th>va17_ratio_word_negatif_vs_word_positif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>66804.000000</td>\n",
       "      <td>66804.000000</td>\n",
       "      <td>66804.000000</td>\n",
       "      <td>66804.000000</td>\n",
       "      <td>66804.000000</td>\n",
       "      <td>66804.000000</td>\n",
       "      <td>66804.000000</td>\n",
       "      <td>66804.000000</td>\n",
       "      <td>66804.000000</td>\n",
       "      <td>66804.000000</td>\n",
       "      <td>66804.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>72.096267</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>9.498039</td>\n",
       "      <td>1.993488</td>\n",
       "      <td>1.862897</td>\n",
       "      <td>51.608496</td>\n",
       "      <td>0.031489</td>\n",
       "      <td>0.432265</td>\n",
       "      <td>0.008551</td>\n",
       "      <td>1.697211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>52.199778</td>\n",
       "      <td>0.499826</td>\n",
       "      <td>0.499504</td>\n",
       "      <td>13.636790</td>\n",
       "      <td>0.080432</td>\n",
       "      <td>3.089907</td>\n",
       "      <td>37.656997</td>\n",
       "      <td>0.042366</td>\n",
       "      <td>1.280961</td>\n",
       "      <td>0.024404</td>\n",
       "      <td>2.911040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.812526</td>\n",
       "      <td>-0.956713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>-0.365992</td>\n",
       "      <td>-0.360203</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>58.000000</td>\n",
       "      <td>-0.130433</td>\n",
       "      <td>-0.128594</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.233663</td>\n",
       "      <td>0.231672</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>356.000000</td>\n",
       "      <td>3.617114</td>\n",
       "      <td>3.890187</td>\n",
       "      <td>414.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>55.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       var_1_bis_taille_reclamation  var_8_distrib_normal_var_1_x_sub_produit  \\\n",
       "count                  66804.000000                              66804.000000   \n",
       "mean                      72.096267                                  0.000020   \n",
       "std                       52.199778                                  0.499826   \n",
       "min                        1.000000                                 -0.812526   \n",
       "25%                       34.000000                                 -0.365992   \n",
       "50%                       58.000000                                 -0.130433   \n",
       "75%                       96.000000                                  0.233663   \n",
       "max                      356.000000                                  3.617114   \n",
       "\n",
       "       var_9_distrib_normal_var_1_x_sub_issue  var_10_nb_XXXX   var_11_nb_$  \\\n",
       "count                            66804.000000    66804.000000  66804.000000   \n",
       "mean                                 0.000019        9.498039      1.993488   \n",
       "std                                  0.499504       13.636790      0.080432   \n",
       "min                                 -0.956713        0.000000      1.000000   \n",
       "25%                                 -0.360203        2.000000      2.000000   \n",
       "50%                                 -0.128594        5.000000      2.000000   \n",
       "75%                                  0.231672       12.000000      2.000000   \n",
       "max                                  3.890187      414.000000      2.000000   \n",
       "\n",
       "       var12_nb_word_dispute  var13_nb_word_different  \\\n",
       "count           66804.000000             66804.000000   \n",
       "mean                1.862897                51.608496   \n",
       "std                 3.089907                37.656997   \n",
       "min                 0.000000                 1.000000   \n",
       "25%                 0.000000                24.000000   \n",
       "50%                 1.000000                41.000000   \n",
       "75%                 2.000000                69.000000   \n",
       "max                55.000000               237.000000   \n",
       "\n",
       "       va14_ratio_word_dispute_vs_all  var15_nb_word_positif  \\\n",
       "count                    66804.000000           66804.000000   \n",
       "mean                         0.031489               0.432265   \n",
       "std                          0.042366               1.280961   \n",
       "min                          0.000000               0.000000   \n",
       "25%                          0.000000               0.000000   \n",
       "50%                          0.020408               0.000000   \n",
       "75%                          0.046512               0.000000   \n",
       "max                          1.000000              44.000000   \n",
       "\n",
       "       va16_ratio_word_positif_vs_all  va17_ratio_word_negatif_vs_word_positif  \n",
       "count                    66804.000000                             66804.000000  \n",
       "mean                         0.008551                                 1.697211  \n",
       "std                          0.024404                                 2.911040  \n",
       "min                          0.000000                                 0.000000  \n",
       "25%                          0.000000                                 0.000000  \n",
       "50%                          0.000000                                 1.000000  \n",
       "75%                          0.000000                                 2.000000  \n",
       "max                          0.666667                                55.000000  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#permet de créer les variables d'observations\n",
    "df_complaint_dataset_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_dataset_1 = std_scaler.fit_transform(df_complaint_dataset_1)\n",
    "df_np_dataset_1 = pd.DataFrame(data=np_dataset_1)\n",
    "np_dataset_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=np_dataset_1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complaint_dataset_2 = df_complaint[['var_1_bis_taille_reclamation',\n",
    "       'var_2_nb_reclamation_company',\n",
    "       'var_3_nb_reclamation_company_sub_product',\n",
    "       'var_4_nb_reclamation_company_sub_issue',\n",
    "       'var_5_nb_reclamation_company_state',\n",
    "       'var_6_nb_reclamation_state_sub_issue',\n",
    "       'var_7_nb_reclamation_state_sub_product', \n",
    "       'var_8_distrib_normal_var_1_x_sub_produit',\n",
    "       'var_9_distrib_normal_var_1_x_sub_issue',\n",
    "       'var_10_nb_XXXX', 'var_11_nb_$']]\n",
    "df_complaint_dataset_2 = df_complaint_dataset_2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66804, 11)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_dataset_2 = std_scaler.fit_transform(df_complaint_dataset_2)\n",
    "df_np_dataset_2 = pd.DataFrame(data=np_dataset_2)\n",
    "np_dataset_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1.323445e-15\n",
       "1    -2.928025e-14\n",
       "2    -1.968632e-14\n",
       "3    -2.431572e-14\n",
       "4    -1.091820e-14\n",
       "5     4.280119e-16\n",
       "6    -1.525083e-14\n",
       "7    -7.078744e-17\n",
       "8    -1.120128e-18\n",
       "9    -3.255940e-15\n",
       "10    8.731286e-16\n",
       "dtype: float64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=np_dataset_2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        1\n",
       "3        0\n",
       "4        0\n",
       "5        1\n",
       "6        1\n",
       "7        1\n",
       "8        0\n",
       "9        1\n",
       "10       0\n",
       "11       1\n",
       "12       0\n",
       "13       0\n",
       "14       1\n",
       "15       0\n",
       "16       0\n",
       "17       1\n",
       "18       0\n",
       "19       0\n",
       "20       1\n",
       "21       0\n",
       "22       0\n",
       "23       0\n",
       "24       0\n",
       "25       0\n",
       "26       0\n",
       "27       0\n",
       "28       0\n",
       "29       0\n",
       "        ..\n",
       "66774    0\n",
       "66775    0\n",
       "66776    0\n",
       "66777    0\n",
       "66778    1\n",
       "66779    0\n",
       "66780    0\n",
       "66781    1\n",
       "66782    0\n",
       "66783    0\n",
       "66784    0\n",
       "66785    1\n",
       "66786    0\n",
       "66787    1\n",
       "66788    0\n",
       "66789    0\n",
       "66790    1\n",
       "66791    0\n",
       "66792    1\n",
       "66793    1\n",
       "66794    1\n",
       "66795    1\n",
       "66796    0\n",
       "66797    0\n",
       "66798    0\n",
       "66799    0\n",
       "66800    1\n",
       "66801    0\n",
       "66802    1\n",
       "66803    0\n",
       "Name: y_to_predict, Length: 66804, dtype: int32"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complaint['y_to_predict'] = np.where(df_complaint['consumer_disputed?'] == 'Yes', 1, 0)\n",
    "y_to_predict = df_complaint['y_to_predict']\n",
    "y_to_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\HOMEWARE\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec le dataset des nouvelles variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=1000, max_depth=4,\n",
    "                              random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(np_dataset_1, y_to_predict, test_size=0.3, random_state=50)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2085"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_binary_rdmf = clf.predict(X_test)\n",
    "sum(pred_binary_rdmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \r\n",
      "[[14710   661]\n",
      " [ 3247  1424]]\n",
      "Accuracy : 0.8050094800918072\n",
      "Precision : 0.6829736211031175\n",
      "Recall : 0.30485977306786555\n"
     ]
    }
   ],
   "source": [
    "_print_eval_model(y_test, pred_binary_rdmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec le deuxième dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=4000, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = RandomForestClassifier(n_estimators=1000, max_depth=4,\n",
    "                              random_state=0)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(np_dataset_2, y_to_predict, test_size=0.3, random_state=50)\n",
    "clf.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_binary_rdmf2 = clf2.predict(X_test2)\n",
    "sum(pred_binary_rdmf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \r\n",
      "[[15371     0]\n",
      " [ 4671     0]]\n",
      "Accuracy : 0.766939427202874\n",
      "Precision : 0.0\n",
      "Recall : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\HOMEWARE\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "_print_eval_model(y_test2, pred_binary_rdmf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Idée de création des paramètres du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_depth':5, 'eta':0.15, 'objective':'binary:logistic', 'eval_metric': 'auc','lambda':1.5, 'silent': 1} #,'gamma':1.5\n",
    "y_output, pred, bst = calcul_model(df_np_dataset_1, 0.3, y_to_predict, param, 600, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idée pour visualiser la classification selon la valeur de la probabilité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-classification : 0.1\n",
      "Confusion matrix : \n",
      "[[6208 9163]\n",
      " [ 336 4335]]\n",
      "Accuracy : 0.5260453048597944\n",
      "Precision : 0.3211586901763224\n",
      "Recall : 0.9280667951188183\n",
      "\n",
      "p-classification : 0.2\n",
      "Confusion matrix : \n",
      "[[10010  5361]\n",
      " [ 1288  3383]]\n",
      "Accuracy : 0.6682466819678675\n",
      "Precision : 0.38689387008234216\n",
      "Recall : 0.7242560479554699\n",
      "\n",
      "p-classification : 0.30000000000000004\n",
      "Confusion matrix : \n",
      "[[12399  2972]\n",
      " [ 2072  2599]]\n",
      "Accuracy : 0.7483285101287297\n",
      "Precision : 0.4665230658768623\n",
      "Recall : 0.5564119032327125\n",
      "\n",
      "p-classification : 0.4\n",
      "Confusion matrix : \n",
      "[[13644  1727]\n",
      " [ 2674  1997]]\n",
      "Accuracy : 0.7804111366131125\n",
      "Precision : 0.5362513426423201\n",
      "Recall : 0.42753157782059514\n",
      "\n",
      "p-classification : 0.5\n",
      "Confusion matrix : \n",
      "[[14382   989]\n",
      " [ 3133  1538]]\n",
      "Accuracy : 0.7943319030036923\n",
      "Precision : 0.6086268302334784\n",
      "Recall : 0.3292656818668379\n",
      "\n",
      "p-classification : 0.6\n",
      "Confusion matrix : \n",
      "[[14809   562]\n",
      " [ 3521  1150]]\n",
      "Accuracy : 0.7962778165851712\n",
      "Precision : 0.6717289719626168\n",
      "Recall : 0.24619995718261614\n",
      "\n",
      "p-classification : 0.7000000000000001\n",
      "Confusion matrix : \n",
      "[[15029   342]\n",
      " [ 3798   873]]\n",
      "Accuracy : 0.7934337890430097\n",
      "Precision : 0.7185185185185186\n",
      "Recall : 0.18689788053949905\n",
      "\n",
      "p-classification : 0.8\n",
      "Confusion matrix : \n",
      "[[15191   180]\n",
      " [ 4047   624]]\n",
      "Accuracy : 0.7890929048997106\n",
      "Precision : 0.7761194029850746\n",
      "Recall : 0.13359023763648042\n",
      "\n",
      "p-classification : 0.9\n",
      "Confusion matrix : \n",
      "[[15300    71]\n",
      " [ 4303   368]]\n",
      "Accuracy : 0.7817583075541363\n",
      "Precision : 0.8382687927107062\n",
      "Recall : 0.07878398629843716\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#méthode d'affichage des valeurs\n",
    "for value_pred in np.arange(0.1,1,0.1):    \n",
    "    pred_binary =  np.where(pred >= value_pred, 1, 0)     \n",
    "    print('p-classification : ' + str(value_pred))\n",
    "    print('Confusion matrix : \\r\\n' + str(metrics.confusion_matrix(y_output.tolist(), pred_binary)))\n",
    "    print('Accuracy : ' + str(metrics.accuracy_score(y_output.tolist(), pred_binary)))\n",
    "    print('Precision : ' + str(metrics.precision_score(y_output.tolist(), pred_binary)))\n",
    "    print('Recall : ' + str(metrics.recall_score(y_output.tolist(), pred_binary)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example si on fait varier le paramètre de la profondeur, l'impact que ça peut avoir.\n",
    "param = {'max_depth':7, 'eta':0.3, 'objective':'binary:logistic', 'eval_metric': 'auc','lambda':1.5, 'silent': 1} #,'gamma':1.5\n",
    "y_output, pred, bst = calcul_model(df_np_dataset_1, 0.3, y_to_predict, param, 600, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-classification : 0.1\n",
      "Confusion matrix : \n",
      "[[7421 7950]\n",
      " [ 656 4015]]\n",
      "Accuracy : 0.5706017363536573\n",
      "Precision : 0.3355620559966569\n",
      "Recall : 0.8595589809462642\n",
      "\n",
      "p-classification : 0.2\n",
      "Confusion matrix : \n",
      "[[10450  4921]\n",
      " [ 1492  3179]]\n",
      "Accuracy : 0.6800219538968166\n",
      "Precision : 0.3924691358024691\n",
      "Recall : 0.6805823164204667\n",
      "\n",
      "p-classification : 0.30000000000000004\n",
      "Confusion matrix : \n",
      "[[12338  3033]\n",
      " [ 2159  2512]]\n",
      "Accuracy : 0.7409440175631175\n",
      "Precision : 0.45302073940486925\n",
      "Recall : 0.5377863412545494\n",
      "\n",
      "p-classification : 0.4\n",
      "Confusion matrix : \n",
      "[[13443  1928]\n",
      " [ 2648  2023]]\n",
      "Accuracy : 0.7716794731064764\n",
      "Precision : 0.5120222728423184\n",
      "Recall : 0.4330978377221152\n",
      "\n",
      "p-classification : 0.5\n",
      "Confusion matrix : \n",
      "[[14124  1247]\n",
      " [ 3014  1657]]\n",
      "Accuracy : 0.7873964674184213\n",
      "Precision : 0.5705922865013774\n",
      "Recall : 0.35474202526225646\n",
      "\n",
      "p-classification : 0.6\n",
      "Confusion matrix : \n",
      "[[14577   794]\n",
      " [ 3359  1312]]\n",
      "Accuracy : 0.7927851511825167\n",
      "Precision : 0.6229819563152896\n",
      "Recall : 0.28088203810747164\n",
      "\n",
      "p-classification : 0.7000000000000001\n",
      "Confusion matrix : \n",
      "[[14912   459]\n",
      " [ 3661  1010]]\n",
      "Accuracy : 0.7944316934437681\n",
      "Precision : 0.6875425459496256\n",
      "Recall : 0.21622778848212373\n",
      "\n",
      "p-classification : 0.8\n",
      "Confusion matrix : \n",
      "[[15111   260]\n",
      " [ 3924   747]]\n",
      "Accuracy : 0.7912383993613412\n",
      "Precision : 0.7418073485600795\n",
      "Recall : 0.1599229287090559\n",
      "\n",
      "p-classification : 0.9\n",
      "Confusion matrix : \n",
      "[[15250   121]\n",
      " [ 4201   470]]\n",
      "Accuracy : 0.7843528589961082\n",
      "Precision : 0.7952622673434856\n",
      "Recall : 0.10062085206593877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for value_pred in np.arange(0.1,1,0.1):\n",
    "    pred_binary =  np.where(pred >= value_pred, 1, 0)     \n",
    "    print('p-classification : ' + str(value_pred))\n",
    "    print('Confusion matrix : \\r\\n' + str(metrics.confusion_matrix(y_output.tolist(), pred_binary)))\n",
    "    print('Accuracy : ' + str(metrics.accuracy_score(y_output.tolist(), pred_binary)))\n",
    "    print('Precision : ' + str(metrics.precision_score(y_output.tolist(), pred_binary)))\n",
    "    print('Recall : ' + str(metrics.recall_score(y_output.tolist(), pred_binary)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_depth':7, 'eta':0.3, 'objective':'binary:logistic', 'eval_metric': 'auc','lambda':1.5, 'silent': 1} #,'gamma':1.5\n",
    "y_output_2, pred_2, bst = calcul_model(df_np_dataset_2, 0.3, y_to_predict, param, 600, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eta in np.arange(0.1:3:0.05):\n",
    "    param = {'max_depth':7, 'eta':eta, 'objective':'binary:logistic', 'eval_metric': 'auc','lambda':1.5, 'silent': 1} #,'gamma':1.5\n",
    "    y_output_2, pred_2, bst = calcul_model(df_np_dataset_2, eta, y_to_predict, param, 600, 50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p : 0.1\n",
      "Confusion matrix : \n",
      "[[6294 9077]\n",
      " [1430 3241]]\n",
      "Accuracy : 0.4757509230615707\n",
      "Precision : 0.26311089462575094\n",
      "Recall : 0.6938557054163991\n",
      "p : 0.2\n",
      "Confusion matrix : \n",
      "[[9996 5375]\n",
      " [2566 2105]]\n",
      "Accuracy : 0.6037820576788744\n",
      "Precision : 0.28141711229946526\n",
      "Recall : 0.4506529651038322\n",
      "p : 0.30000000000000004\n",
      "Confusion matrix : \n",
      "[[12160  3211]\n",
      " [ 3274  1397]]\n",
      "Accuracy : 0.6764294980540864\n",
      "Precision : 0.3031684027777778\n",
      "Recall : 0.2990794262470563\n",
      "p : 0.4\n",
      "Confusion matrix : \n",
      "[[13454  1917]\n",
      " [ 3740   931]]\n",
      "Accuracy : 0.7177427402454845\n",
      "Precision : 0.32689606741573035\n",
      "Recall : 0.19931492185827446\n",
      "p : 0.5\n",
      "Confusion matrix : \n",
      "[[14242  1129]\n",
      " [ 4061   610]]\n",
      "Accuracy : 0.7410438080031932\n",
      "Precision : 0.35077630822311673\n",
      "Recall : 0.13059302076643117\n",
      "p : 0.6\n",
      "Confusion matrix : \n",
      "[[14760   611]\n",
      " [ 4270   401]]\n",
      "Accuracy : 0.7564614309949107\n",
      "Precision : 0.39624505928853754\n",
      "Recall : 0.0858488546349818\n",
      "p : 0.7000000000000001\n",
      "Confusion matrix : \n",
      "[[15081   290]\n",
      " [ 4434   237]]\n",
      "Accuracy : 0.7642949805408642\n",
      "Precision : 0.4497153700189753\n",
      "Recall : 0.05073859987154785\n",
      "p : 0.8\n",
      "Confusion matrix : \n",
      "[[15242   129]\n",
      " [ 4528   143]]\n",
      "Accuracy : 0.7676379602834048\n",
      "Precision : 0.5257352941176471\n",
      "Recall : 0.030614429458360095\n",
      "p : 0.9\n",
      "Confusion matrix : \n",
      "[[15334    37]\n",
      " [ 4610    61]]\n",
      "Accuracy : 0.768136912483784\n",
      "Precision : 0.6224489795918368\n",
      "Recall : 0.013059302076643117\n"
     ]
    }
   ],
   "source": [
    "for value_pred_2 in np.arange(0.1,1,0.1):\n",
    "    pred_binary2 =  np.where(pred_2 >= value_pred_2, 1, 0)  \n",
    "    print('p : ' + str(value_pred_2))\n",
    "    _print_eval_model(y_output, pred_binary2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question : Quelle variable pourrions nous créer? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question : Comment pourrions-nous optimiser le modèle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Méthode naive de recherche de valeur qui optimise la précision pour un paramètre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_mode_optim = []\n",
    "l_xgb_accuracy = []\n",
    "l_xgb_precision = []\n",
    "l_xgb_recall = []\n",
    "l_best_param = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_precision = 0\n",
    "xgb_accuracy = 0\n",
    "xgb_recall = 0\n",
    "best_param = 0\n",
    "for value_param in range(1,10,1):\n",
    "    print(value_param)\n",
    "    param = {'max_depth':value_param, 'eta':0.3, 'silent': 1, 'objective':'binary:logistic', 'eval_metric': 'auc','lambda':1.5} #,'gamma':1.5\n",
    "    y_output, pred, bst = calcul_model(df_np_dataset, 0.3, y_to_predict, param, 300, 50)\n",
    "    pred_binary =  np.where(pred > 0.5, 1, 0) \n",
    "    if metrics.precision_score(y_output.tolist(), pred_binary) > xgb_precision:\n",
    "        xgb_accuracy = metrics.accuracy_score(y_output.tolist(), pred_binary)\n",
    "        precision_confusion_metrics = metrics.confusion_matrix(y_output.tolist(), pred_binary)\n",
    "        best_param = value_param\n",
    "        xgb_precision = metrics.precision_score(y_output.tolist(), pred_binary)\n",
    "        xgb_recall = metrics.recall_score(y_output.tolist(), pred_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_mode_optim.append('precision_max_depth')\n",
    "l_xgb_accuracy.append(xgb_accuracy)\n",
    "l_best_param.append(best_param)\n",
    "l_xgb_precision.append(xgb_precision)\n",
    "l_xgb_recall.append(xgb_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_precision = 0\n",
    "xgb_accuracy = 0\n",
    "xgb_recall = 0\n",
    "best_param = 0\n",
    "xgb_optim_value = -9999999\n",
    "for value_param in range(1,9,1):\n",
    "    print(value_param)\n",
    "    param = {'max_depth':value_param, 'eta':0.3, 'silent': 1, 'objective':'binary:logistic', 'eval_metric': 'auc','lambda':1.5} #,'gamma':1.5\n",
    "    y_output, pred, bst = calcul_model(df_complaint_dataset, 0.3, y_to_predict, param, 300, 50)\n",
    "    pred_binary =  np.where(pred > 0.5, 1, 0) \n",
    "    if _execute_model_utilitie(y_output, pred_binary) >  xgb_optim_value:\n",
    "        xgb_optim_value = _execute_model_utilitie(y_output, pred_binary)\n",
    "        optimum_confusion_metrics = metrics.confusion_matrix(y_output.tolist(), pred_binary)\n",
    "        xgb_accuracy = metrics.accuracy_score(y_output.tolist(), pred_binary)\n",
    "        best_param = value_param\n",
    "        xgb_precision = metrics.precision_score(y_output.tolist(), pred_binary)\n",
    "        xgb_recall = metrics.recall_score(y_output.tolist(), pred_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_mode_optim.append('optim_max_depth')\n",
    "l_xgb_accuracy.append(xgb_accuracy)\n",
    "l_best_param.append(best_param)\n",
    "l_xgb_precision.append(xgb_precision)\n",
    "l_xgb_recall.append(xgb_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_xgb_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_xgb_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimum_confusion_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "\n",
    "xgb_precision = 0\n",
    "xgb_accuracy = 0\n",
    "xgb_recall = 0\n",
    "best_param = 0\n",
    "xgb_optim_value = -9999999\n",
    "for value_param in np..arange(0.05,0.5,0.05):\n",
    "    print(value_param)\n",
    "    param = {'max_depth': 7, 'eta':value_param, 'silent': 1, 'objective':'binary:logistic', 'eval_metric': 'auc','lambda':1.5} #,'gamma':1.5\n",
    "    y_output, pred, bst = calcul_model(df_complaint_dataset, 0.3, y_to_predict, param, 300, 50)\n",
    "    pred_binary =  np.where(pred > 0.5, 1, 0) \n",
    "    if _execute_model_utilitie(y_output, pred_binary) >  xgb_optim_value:\n",
    "        xgb_optim_value = _execute_model_utilitie(y_output, pred_binary)\n",
    "        optimum_confusion_metrics = metrics.confusion_matrix(y_output.tolist(), pred_binary)\n",
    "        xgb_accuracy = metrics.accuracy_score(y_output.tolist(), pred_binary)\n",
    "        best_param = value_param\n",
    "        xgb_precision = metrics.precision_score(y_output.tolist(), pred_binary)\n",
    "        xgb_recall = metrics.recall_score(y_output.tolist(), pred_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_confusion_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(y_output.tolist(), pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = _execute_model_utilitie(y_output, pred_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('accuracy : ' + str(xgb_accuracy))\n",
    "print ('precision : ' + str(xgb_precision))\n",
    "print ('recall : ' + str(xgb_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_y0_pred0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.where(pred > 0.5, 1, 0))\n",
    "# y_output.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pred_binary * y_output) +  - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20 * np.sum(pred_binary) - np.sum(pred_binary * y_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = pd.DataFrame(np.transpose([np.array(y_output),pred_binary, pred]), columns=['y', 'y_pred_0.5', 'y_pred_log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.to_csv('D:/Mission/Dauphine/2019/20190204/_data/y_pred.csv',sep=';',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(X, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Dropout, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset, y, test_size=test_size, random_state=rnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nombre de reviews pour l'entraînement : \", np.shape(x_train))\n",
    "print(\"Nombre de reviews pour le test : \", np.shape(x_test))\n",
    "vocab_size = 5000\n",
    "train = reduce_vocab(x_train,vocab_size)\n",
    "test = reduce_vocab(x_test,vocab_size)\n",
    "print(\"Nombre de reviews pour l'entraînement : \", np.shape(x_train))\n",
    "print(\"Nombre de reviews pour le test : \", np.shape(x_test))\n",
    "vocab_size = 5000\n",
    "train = reduce_vocab(x_train,vocab_size)\n",
    "test = reduce_vocab(x_test,vocab_size)\n",
    "lens = np.array(list(map(len, train)))\n",
    "print('Maximum text length:', lens.max(),' -- Minimum length:', lens.min(), '-- Mean length of text:',lens.mean())\n",
    "# Homogénéisation de la taille des commentaires par padding\n",
    "seq_len = 215\n",
    "trn = sequence.pad_sequences(train, maxlen = seq_len,value=0)\n",
    "test = sequence.pad_sequences(test, maxlen = seq_len,value=0)\n",
    "model = CNN()\n",
    "model.fit(trn, y_train, validation_data=(test, y_test), epochs=6, batch_size=64)\n",
    "scores = model.evaluate(test,y_test,verbose=0)\n",
    "print('loss: ', scores[0],'- accuracy: ', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _execute_model_utilitie(y_output, y_pred):\n",
    "    nb_y1_pred1 = np.sum(pred_binary * y_output)\n",
    "    nb_y0_pred1 = np.sum(pred_binary) - nb_y1_pred1\n",
    "    nb_y1_pred0 = sum(y_output) - nb_y1_pred1\n",
    "    nb_y0_pred0 =  len(y_output) - sum(y_output) - nb_y0_pred1\n",
    "    return nb_y1_pred1 * 100 + nb_y0_pred0 - 25 * nb_y0_pred1 - 50 * nb_y1_pred0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_eval_model(y_output, y_pred):\n",
    "    print('Confusion matrix : \\r\\n' + str(metrics.confusion_matrix(y_output.tolist(), y_pred)))\n",
    "    print('Accuracy : ' + str(metrics.accuracy_score(y_output.tolist(), y_pred)))\n",
    "    print('Precision : ' + str(metrics.precision_score(y_output.tolist(), y_pred)))\n",
    "    print('Recall : ' + str(metrics.recall_score(y_output.tolist(), y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _lemmatize(x):\n",
    "#     l_lemmatized = []\n",
    "#     for word in tokenizer.tokenize(x):\n",
    "#         word_postag = nltk.pos_tag(tokenizer.tokenize(word))[0]\n",
    "#         if 'VB' in word_postag[1]:\n",
    "#             l_lemmatized.append(lemmatizer.lemmatize(word_postag[0], 'v'))\n",
    "#         elif 'NN' in word_postag[1]:\n",
    "#             l_lemmatized.append(lemmatizer.lemmatize(word_postag[0], 'n'))\n",
    "#         elif 'JJ' in word_postag[1]:\n",
    "#             l_lemmatized.append(lemmatizer.lemmatize(word_postag[0], 'a'))\n",
    "#         elif 'RB' in word_postag[1]:\n",
    "#             try:\n",
    "#                 if len(wn.synset(word_postag[0]+'.r.1').lemmas()[0].pertainyms()) > 0:                \n",
    "#                     l_lemmatized.append(wn.synset(word_postag[0]+'.r.1').lemmas()[0].pertainyms()[0].name())\n",
    "#                 else:\n",
    "#                     l_lemmatized.append(word_postag[0])\n",
    "#             except:\n",
    "#                 l_lemmatized.append(word_postag[0])\n",
    "#                 continue                    \n",
    "#         else: \n",
    "#             l_lemmatized.append(word_postag[0])\n",
    "#     return ' '.join(l_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _lemmatize(x):\n",
    "    l_lemmatized = []\n",
    "    for word in tokenizer.tokenize(x):\n",
    "        wn_word = wn.synsets(word)\n",
    "        if len(wn_word) > 0:\n",
    "            l_lemmatized.append(lemmatizer.lemmatize(word, wn_word[0].pos()))\n",
    "        else: \n",
    "            l_lemmatized.append(word)\n",
    "    return ' '.join(l_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nb_word_dispute(x):\n",
    "    return len(set(tokenizer.tokenize(x)).intersection(set_disputed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 32, input_length=seq_len))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64, 5, padding='same', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling1D())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['recall', 'precision'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(data_ids, len_data):\n",
    "    '''\n",
    "    Correspondance entre les mots et leur codage via un entier\n",
    "    '''\n",
    "    word_to_id = imdb.get_word_index()\n",
    "    word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "    word_to_id[\"<PAD>\"] = 0\n",
    "    word_to_id[\"<START>\"] = 1\n",
    "    word_to_id[\"<UNK>\"] = 2\n",
    "    id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "\n",
    "    text = []\n",
    "    for i in range(len_data):\n",
    "        text.append(' '.join(id_to_word[id] for id in data_ids[i]))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_vocab(data,vocab_size):\n",
    "    '''\n",
    "    Supprime les mots qui ne font pas partie du dictionnaire des vocab_size mots les plus utilisés\n",
    "    '''\n",
    "    data = [list(filter(lambda x: x <=vocab_size-1, list_)) for list_ in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_txt(data):\n",
    "    '''\n",
    "    Correspondance entre le type d'avis et son codage binaire\n",
    "    '''\n",
    "    values = []\n",
    "    for idx in range(len(data)):\n",
    "        if data[idx] == [1]:\n",
    "            values.append('Positive')\n",
    "        else:\n",
    "            values.append('Negative')\n",
    "    return values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "218px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
